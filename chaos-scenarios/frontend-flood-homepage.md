# Frontend Flood Homepage - Service Overload Chaos Test

**Test Type:** Resource Exhaustion / Service Overload  
**Target Service:** Frontend (Python/Flask)  
**Trigger:** Feature flag `loadGeneratorFloodHomepage`  
**Duration:** Configurable (recommend 5-10 minutes)  
**Difficulty:** ‚≠ê Easy

---

## üéØ **What This Test Does**

Floods the frontend service with a massive volume of homepage requests, causing:
- CPU and memory exhaustion
- Connection pool saturation
- Increased latency and timeouts
- Potential service degradation or crashes

**NOT** an Envoy rate limiting test - this is pure service overload.

---

## üìã **Prerequisites**

- OpenTelemetry Demo running
- Load generator active (Locust)
- Access to FlagD UI (http://localhost:4000)
- Honeycomb or similar observability platform

---

## ‚öôÔ∏è **Test Configuration**

### Feature Flag Settings

**Flag:** `loadGeneratorFloodHomepage`

**Variants:**
- `on: 100` - Each user makes 100 sequential homepage requests per cycle
- `off: 0` - Normal behavior (no flooding)

**Recommended Settings:**

| Users | Flood Count | Total Requests/Cycle | Impact Level |
|-------|-------------|---------------------|--------------|
| 10    | 100         | 1,000               | ‚ö†Ô∏è Light     |
| 25    | 100         | 2,500               | üü° Moderate  |
| 50    | 100         | 5,000               | üü† Heavy     |
| 100   | 100         | 10,000              | üî¥ Severe    |

---

## üìù **Step-by-Step Execution**

### Step 1: Check Baseline Frontend Status

```bash
# Check frontend pod status
kubectl get pods -n otel-demo -l app.kubernetes.io/component=frontend

# Check current resource usage
kubectl top pod -n otel-demo -l app.kubernetes.io/component=frontend

# Check current request rate (if available)
kubectl logs -n otel-demo -l app.kubernetes.io/component=frontend --tail=50
```

**Expected Baseline:**
```
CPU: 20-50m (low)
Memory: 80-120Mi
Status: 2/2 Running
Errors: Minimal or none
```

---

### Step 2: Configure Load Generator

1. **Access Locust UI:**
   ```
   http://localhost:8089
   ```

2. **Set User Count:**
   - For first test: **25 users** (moderate impact)
   - Spawn rate: **5** users per second
   - Runtime: **10 minutes** (600 seconds)

3. **Start Load Generator** (but don't enable flag yet)

---

### Step 3: Enable Feature Flag

1. **Access FlagD UI:**
   ```
   http://localhost:4000
   ```

2. **Find `loadGeneratorFloodHomepage` flag**

3. **Configure:**
   - Set **defaultVariant** to `on`
   - Confirm `on` variant value is `100`
   - Save changes

4. **Verify flag is active:**
   ```bash
   kubectl logs -n otel-demo -l app.kubernetes.io/component=flagd --tail=20
   ```

---

### Step 4: Monitor Frontend Impact

#### Real-Time CPU/Memory Monitoring

```bash
# Watch frontend resources (updates every 5 seconds)
watch -n 5 'kubectl top pod -n otel-demo -l app.kubernetes.io/component=frontend'
```

**Expected Progression:**
```
Time    CPU      Memory    Status
0m      30m      100Mi     Normal
1m      150m     180Mi     Load increasing
2m      350m     250Mi     Heavy load
3m      500m+    300Mi+    Overload
```

#### Watch Frontend Logs for Errors

```bash
# Stream frontend logs
kubectl logs -n otel-demo -l app.kubernetes.io/component=frontend -f | grep -i "error\|timeout\|failed"
```

**Expected Errors:**
```
Error: Request timeout
Error: Connection pool exhausted
Error: Too many requests
OSError: [Errno 24] Too many open files
```

#### Check Active Connections

```bash
# Check number of frontend pods and their connections
kubectl exec -n otel-demo <frontend-pod> -c frontend -- \
  sh -c 'netstat -an | grep ESTABLISHED | wc -l'
```

---

### Step 5: Observe Blast Radius

#### Check Dependent Services

**Product-Catalog (called by frontend):**
```bash
kubectl logs -n otel-demo -l app.kubernetes.io/component=product-catalog --tail=50
```

**Expected:**
- Increased request volume
- Possible timeout errors from frontend
- Higher latency

**Checkout Service:**
```bash
kubectl logs -n otel-demo -l app.kubernetes.io/component=checkout --tail=50
```

**Expected:**
- Increased traffic during flood
- Potential connection issues

---

### Step 6: Stop Test and Verify Recovery

#### Disable Feature Flag

1. Go to FlagD UI: http://localhost:4000
2. Set `loadGeneratorFloodHomepage` defaultVariant to `off`
3. Save changes

#### Verify Recovery

```bash
# Check frontend returns to normal
kubectl top pod -n otel-demo -l app.kubernetes.io/component=frontend

# Verify no more errors
kubectl logs -n otel-demo -l app.kubernetes.io/component=frontend --tail=20 --since=1m

# Check pod is healthy
kubectl get pods -n otel-demo -l app.kubernetes.io/component=frontend
```

**Expected Recovery:**
```
CPU: Returns to 20-50m within 30 seconds
Memory: Slowly decreases to baseline
Errors: Stop appearing
Status: 2/2 Running (no restarts needed)
```

---

## üìä **Expected Results**

### Frontend Service Impact

| Metric | Before | During Flood | Impact |
|--------|--------|-------------|--------|
| **CPU Usage** | 20-50m | 400-800m | 8-16x increase |
| **Memory Usage** | 100Mi | 250-400Mi | 2.5-4x increase |
| **Request Latency (P95)** | 50ms | 500-2000ms | 10-40x increase |
| **Error Rate** | <0.1% | 5-20% | 50-200x increase |
| **Active Connections** | 10-20 | 100-300 | 5-15x increase |

### Error Types Observed

1. **Connection Errors:**
   ```
   ConnectionError: HTTPConnectionPool(host='frontend', port=8080): Max retries exceeded
   ```

2. **Timeout Errors:**
   ```
   TimeoutError: Request to / timed out after 30 seconds
   ```

3. **Resource Exhaustion:**
   ```
   OSError: [Errno 24] Too many open files
   MemoryError: Unable to allocate array
   ```

4. **HTTP Status Codes:**
   - 500 Internal Server Error (service overwhelmed)
   - 503 Service Unavailable (can't handle requests)
   - 504 Gateway Timeout (response too slow)

---

## üîç **Blast Radius Analysis**

### Service Dependency Chain

```
Load Generator (Locust)
    ‚Üì 100 requests per user per cycle
Frontend (Python/Flask)
    ‚Üì Calls product-catalog, checkout, cart, etc.
Product-Catalog (Go)
    ‚Üì Increased query volume
PostgreSQL
    ‚Üì More database queries
Potential cascade failures
```

### Impact by Service

**Frontend (Primary Target):**
- üî¥ **Direct Impact** - Resource exhaustion
- CPU saturation (400-800m)
- Memory pressure (250-400Mi)
- Connection pool exhaustion
- High error rate (5-20%)

**Product-Catalog (Secondary):**
- üü° **Indirect Impact** - Increased call volume
- Higher query rate
- Potential timeout errors
- Database connection pressure

**Cart/Checkout Services (Tertiary):**
- üü¢ **Minor Impact** - Increased traffic
- Mostly handles increased volume fine
- Potential timeout on slow responses

**Database (Quaternary):**
- üü¢ **Minimal Impact** - Query volume increase
- No direct stress
- May see increased connections

---

## üìà **Honeycomb Queries**

### Frontend Request Rate

```
WHERE: service.name = "frontend" AND name = "GET /"
VISUALIZE: COUNT
GROUP BY: time(1m)
TIME RANGE: Last 15 minutes
```

**Expected:** Spike from 50-100 req/min to 5,000-10,000 req/min

### Frontend Error Rate

```
WHERE: service.name = "frontend" AND error EXISTS
VISUALIZE: COUNT, COUNT / COUNT(service.name = "frontend") * 100 AS "Error %"
BREAKDOWN: error
GROUP BY: time(1m)
TIME RANGE: Last 15 minutes
```

**Expected:** Error rate increases from <0.1% to 5-20%

### Frontend Latency

```
WHERE: service.name = "frontend" AND name = "GET /"
VISUALIZE: P50(duration_ms), P95(duration_ms), P99(duration_ms)
GROUP BY: time(1m)
TIME RANGE: Last 15 minutes
```

**Expected:**
- P50: 20ms ‚Üí 200ms (10x)
- P95: 50ms ‚Üí 1000ms (20x)
- P99: 100ms ‚Üí 3000ms+ (30x+)

### Frontend HTTP Status Codes

```
WHERE: service.name = "frontend"
VISUALIZE: COUNT
BREAKDOWN: http.status_code
GROUP BY: time(1m)
TIME RANGE: Last 15 minutes
```

**Expected:** Increase in 500, 503, 504 status codes

---

## üéØ **Key Differences from Other Tests**

| Test | Target | Method | Error Type |
|------|--------|--------|------------|
| **Frontend Flood** | Frontend service | High request volume | 500/503/timeout |
| **Connection Exhaustion** | PostgreSQL | Hold connections | "too many clients" |
| **IOPS Pressure** | PostgreSQL | Heavy I/O | EOF, "shutting down" |
| **Kafka Queue** | Checkout/Accounting | Message flood | Kafka lag, OOM |

**Unique Aspect:** This is the only test that directly targets the frontend service with HTTP request volume.

---

## ‚ö†Ô∏è **Important Notes**

### Frontend Service Limits

**Memory Limit:**
```yaml
resources:
  limits:
    memory: 1000Mi  # Frontend can handle up to 1GB
```

**Risk Levels:**
- **25 users √ó 100 requests** = ‚ö†Ô∏è Safe (will not OOM)
- **50 users √ó 100 requests** = üü° Moderate (high load, no OOM)
- **100 users √ó 100 requests** = üî¥ Risky (may approach memory limit)

### Recovery Time

- **CPU:** Returns to baseline in 30-60 seconds
- **Memory:** Gradually decreases over 2-5 minutes
- **Connections:** Close within 1-2 minutes
- **No restart needed** in most cases

### When to Stop Early

Stop the test immediately if:
- ‚ùå Frontend pod shows `OOMKilled` status
- ‚ùå Frontend becomes completely unresponsive (no logs for 60+ seconds)
- ‚ùå Other services start cascading failures
- ‚ùå Kubernetes evicts the frontend pod

---

## üîß **Troubleshooting**

### Frontend Not Showing Load

**Check:**
1. Is the load generator running? (http://localhost:8089)
2. Is the flag actually enabled? (http://localhost:4000)
3. Are users actually spawned in Locust?

```bash
kubectl logs -n otel-demo -l app.kubernetes.io/component=load-generator --tail=50
```

### Frontend OOM Killed

**Recovery:**
```bash
# Frontend will auto-restart, but you can force it
kubectl rollout restart deployment frontend -n otel-demo

# Verify recovery
kubectl get pods -n otel-demo -l app.kubernetes.io/component=frontend
```

### Test Impact Too Severe

**Reduce load:**
1. Stop Locust (http://localhost:8089 ‚Üí Stop)
2. Disable flag (http://localhost:4000 ‚Üí `loadGeneratorFloodHomepage` ‚Üí off)
3. Reduce users: 100 ‚Üí 50 ‚Üí 25
4. Restart with lower settings

---

## üéì **Learning Outcomes**

After completing this test, you will have observed:

1. ‚úÖ **Service overload patterns** - How services behave under extreme load
2. ‚úÖ **Resource exhaustion** - CPU, memory, connection limits
3. ‚úÖ **Error propagation** - How frontend errors affect dependent services
4. ‚úÖ **Recovery behavior** - How services self-heal after load reduction
5. ‚úÖ **Observability effectiveness** - Can you detect and diagnose the issue?
6. ‚úÖ **Resource limits** - Understanding service capacity limits

---

## üìö **Related Tests**

- **Connection Exhaustion:** `infra/postgres-chaos/docs/CONNECTION-EXHAUSTION-TEST-RESULTS.md`
- **IOPS Pressure:** `infra/postgres-chaos/docs/IOPS-BLAST-RADIUS-TEST-RESULTS.md`
- **Kafka Queue Problems:** Feature flag `kafkaQueueProblems`
- **Memory Leak (Email):** Feature flag `emailMemoryLeak`

---

## ‚úÖ **Success Criteria**

The test is successful when you can demonstrate:

1. ‚úÖ **Load increase visible** - Frontend CPU/memory spike observable
2. ‚úÖ **Errors generated** - 500/503/timeout errors appearing
3. ‚úÖ **Latency degradation** - P95/P99 latency increases significantly
4. ‚úÖ **Monitoring works** - Honeycomb shows the impact clearly
5. ‚úÖ **Recovery verified** - Service returns to normal after flag disabled
6. ‚úÖ **No permanent damage** - No data loss, no restart needed

---

**Test Type:** Frontend Service Overload  
**Difficulty:** ‚≠ê Easy  
**Duration:** 5-10 minutes  
**Status:** ‚úÖ Safe for demo environments  
**Recovery:** Automatic, no intervention needed

