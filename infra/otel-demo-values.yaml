components:
  load-generator:
    envOverrides:
      - name: LOCUST_BROWSER_TRAFFIC_ENABLED
        value: "true"
    resources:
      limits:
        memory: 4Gi
        cpu: 2000m
      requests:
        memory: 2Gi
        cpu: 1000m

  accounting:
    resources:
      limits:
        memory: 256Mi
      requests:
        memory: 128Mi

  postgresql:
    resources:
      limits:
        memory: 300Mi
      requests:
        memory: 300Mi
    # PostgreSQL logging configuration for lock and performance visibility
    envOverrides:
      - name: POSTGRES_LOGGING_COLLECTOR
        value: "on"
      - name: POSTGRES_LOG_DIRECTORY
        value: "/var/log/postgresql"
      - name: POSTGRES_LOG_FILENAME
        value: "postgresql.log"
      - name: POSTGRES_LOG_LOCK_WAITS
        value: "on"
      - name: POSTGRES_DEADLOCK_TIMEOUT
        value: "1s"
      - name: POSTGRES_LOG_MIN_DURATION_STATEMENT
        value: "1000"  # Log queries >1 second
      - name: POSTGRES_LOG_LINE_PREFIX
        value: "%t [%p] %u@%d "
      - name: POSTGRES_LOG_CHECKPOINTS
        value: "on"
      - name: POSTGRES_LOG_CONNECTIONS
        value: "on"
      - name: POSTGRES_LOG_DISCONNECTIONS
        value: "on"
    # Optional: Reduce shared_buffers for IOPS pressure demo
    # Lower cache = more disk reads = visible IOPS issues
    # - name: POSTGRES_SHARED_BUFFERS
    #   value: "32MB"  # Default: 128MB. Use 32MB or 64MB for chaos demo

  checkout:
    resources:
      limits:
        memory: 100Mi
      requests:
        memory: 50Mi

  # frontend-proxy:
  #   Rate limiting is ENABLED by default (500 req/min = 8.3 req/sec)
  #   Config: 500 max_tokens, 500 tokens_per_fill, 60s fill_interval
  #
  #   To modify rate limits:
  #   1. Edit envoy-config ConfigMap: kubectl edit configmap envoy-config -n otel-demo
  #   2. Restart frontend-proxy: kubectl rollout restart deployment/frontend-proxy -n otel-demo
  #
  #   See infra/FRONTEND-PROXY-RATE-LIMITING.md for details and examples

opentelemetry-collector:
  clusterRole:
    create: true
    rules:
      - apiGroups: [""]
        resources: ["nodes/stats", "nodes/proxy"]
        verbs: ["get", "list", "watch"]
      - apiGroups: [""]
        resources: ["nodes", "pods", "replicationcontrollers", "resourcequotas", "services", "events"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["events.k8s.io"]
        resources: ["events"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["batch"]
        resources: ["jobs", "cronjobs"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["autoscaling"]
        resources: ["horizontalpodautoscalers"]
        verbs: ["get", "list", "watch"]
  extraEnvs:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  config:
    receivers:
      kubeletstats:
        collection_interval: 20s
        auth_type: "serviceAccount"
        endpoint: "https://${env:K8S_NODE_NAME}:10250"
        insecure_skip_verify: true
        metric_groups:
          - pod
          - node
          - container
      k8s_cluster:
        collection_interval: 30s
        node_conditions_to_report: ["Ready", "MemoryPressure", "DiskPressure"]
        allocatable_types_to_report: ["cpu", "memory"]
        metrics:
          k8s.container.restarts:
            enabled: true
          k8s.pod.phase:
            enabled: true
          k8s.deployment.desired:
            enabled: true
          k8s.deployment.available:
            enabled: true
      prometheus:
        config:
          scrape_configs:
            - job_name: 'istio-mesh'
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                # Only scrape pods with istio-proxy container
                - source_labels: [__meta_kubernetes_pod_container_name]
                  action: keep
                  regex: istio-proxy
                # Get pod IP and set to port 15020
                - source_labels: [__meta_kubernetes_pod_ip]
                  action: replace
                  target_label: __address__
                  replacement: $1:15020
                # Add namespace label
                - source_labels: [__meta_kubernetes_namespace]
                  target_label: namespace
                # Add pod name label
                - source_labels: [__meta_kubernetes_pod_name]
                  target_label: pod
                # Add pod labels as metric labels
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
              metrics_path: /stats/prometheus
              scrape_interval: 30s
            
            - job_name: 'istiod'
              kubernetes_sd_configs:
                - role: pod
                  namespaces:
                    names:
                      - istio-system
              relabel_configs:
                # Only scrape istiod pods
                - source_labels: [__meta_kubernetes_pod_label_app]
                  action: keep
                  regex: istiod
                # Get pod IP and set to port 15014 (istiod metrics port)
                - source_labels: [__meta_kubernetes_pod_ip]
                  action: replace
                  target_label: __address__
                  replacement: $1:15014
                # Add namespace label
                - source_labels: [__meta_kubernetes_namespace]
                  target_label: namespace
                # Add pod name label
                - source_labels: [__meta_kubernetes_pod_name]
                  target_label: pod
                # Add pod labels as metric labels
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
              metrics_path: /metrics
              scrape_interval: 30s
      k8sobjects:
        auth_type: serviceAccount
        objects:
          - name: pods
            mode: watch
            group: ""
            exclude_watch_type:
              - DELETED
          - name: nodes
            mode: watch
            group: ""
          - name: events
            mode: watch
            group: events.k8s.io
            namespaces: [otel-demo]
    processors:
      resource/prometheus:
        attributes:
          - key: job
            action: upsert
            from_attribute: service.name
      k8sattributes:
        passthrough: true
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.statefulset.name
            - k8s.daemonset.name
            - k8s.cronjob.name
            - k8s.job.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.pod.start_time
          labels:
            - tag_name: app.kubernetes.io/name
              key: app.kubernetes.io/name
              from: pod
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: connection
      resource/k8s_service_name:
        attributes:
          - key: service.name
            from_attribute: k8s.pod.labels.app.kubernetes.io/name
            action: upsert
      resource/k8s_events:
        attributes:
          - key: honeycomb.dataset
            value: k8s-events
            action: upsert
      transform/k8s_events:
        log_statements:
          - context: log
            statements:
              # Extract Event object fields (for Started, BackOff, etc.)
              - set(attributes["k8s.object.kind"], body["object"]["kind"])
              - set(attributes["k8s.event.reason"], body["object"]["reason"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.event.type"], body["object"]["type"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.event.message"], body["object"]["note"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.pod.name"], body["object"]["regarding"]["name"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.namespace.name"], body["object"]["regarding"]["namespace"]) where body["object"]["kind"] == "Event"
              
              # Extract Pod object fields (for OOMKilled, crashes, restarts)
              - set(attributes["k8s.pod.name"], body["object"]["metadata"]["name"]) where body["object"]["kind"] == "Pod"
              - set(attributes["k8s.namespace.name"], body["object"]["metadata"]["namespace"]) where body["object"]["kind"] == "Pod"
              - set(attributes["k8s.pod.phase"], body["object"]["status"]["phase"]) where body["object"]["kind"] == "Pod"

              # Extract Node object fields (for node crashes and status changes)
              - set(attributes["k8s.node.name"], body["object"]["metadata"]["name"]) where body["object"]["kind"] == "Node"
              - set(attributes["k8s.node.condition.ready"], body["object"]["status"]["conditions"][0]["status"]) where body["object"]["kind"] == "Node" and body["object"]["status"]["conditions"][0]["type"] == "Ready"
              - set(attributes["k8s.node.condition.memory_pressure"], body["object"]["status"]["conditions"][1]["status"]) where body["object"]["kind"] == "Node" and body["object"]["status"]["conditions"][1]["type"] == "MemoryPressure"
              - set(attributes["k8s.node.condition.disk_pressure"], body["object"]["status"]["conditions"][2]["status"]) where body["object"]["kind"] == "Node" and body["object"]["status"]["conditions"][2]["type"] == "DiskPressure"

              # Extract container termination info (OOMKilled, Error, etc.)
              - set(attributes["k8s.container.name"], body["object"]["status"]["containerStatuses"][0]["name"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"] != nil
              - set(attributes["k8s.container.restart_count"], body["object"]["status"]["containerStatuses"][0]["restartCount"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"] != nil
              - set(attributes["k8s.container.ready"], body["object"]["status"]["containerStatuses"][0]["ready"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"] != nil
              
              # Current state
              - set(attributes["k8s.container.state"], body["object"]["status"]["containerStatuses"][0]["state"]["waiting"]["reason"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["state"]["waiting"] != nil
              - set(attributes["k8s.container.state"], body["object"]["status"]["containerStatuses"][0]["state"]["terminated"]["reason"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["state"]["terminated"] != nil
              - set(attributes["k8s.container.state"], "Running") where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["state"]["running"] != nil
              
              # Last termination (OOMKilled shows up here!)
              - set(attributes["k8s.container.last_terminated.reason"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["reason"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
              - set(attributes["k8s.container.last_terminated.exit_code"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["exitCode"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
              - set(attributes["k8s.container.last_terminated.message"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["message"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
              - set(attributes["k8s.container.last_terminated.finished_at"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["finishedAt"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
    exporters:
      otlp/honeycomb:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-team: f1xvILrABDNS1T6HKFvaCB
          x-honeycomb-dataset: opentelemetry-demo
      otlp/honeycomb_events:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-team: f1xvILrABDNS1T6HKFvaCB
          x-honeycomb-dataset: k8s-events
    service:
      pipelines:
        logs:
          exporters:
          - otlp/honeycomb
          - debug
        logs/k8sevents:
          receivers:
          - k8sobjects
          processors:
          - resource/k8s_events
          - transform/k8s_events
          exporters:
          - otlp/honeycomb_events
          - debug
        metrics:
          receivers:
          - spanmetrics
          - kubeletstats
          - k8s_cluster
          - postgresql
          - prometheus
          processors:
          - k8sattributes
          - resource/k8s_service_name
          - resource/prometheus
          - attributes/prometheus
          exporters:
          - otlp/honeycomb
          - debug
        traces:
          exporters:
          - otlp/honeycomb
          - spanmetrics
          - debug
