components:
  load-generator:
    replicas: 1
    envOverrides:
      - name: LOCUST_BROWSER_TRAFFIC_ENABLED
        value: "false"
    resources:
      limits:
        memory: 4Gi
        cpu: 2000m
        ephemeral-storage: "2Gi"
      requests:
        memory: 2Gi
        cpu: 1000m
        ephemeral-storage: "1Gi"

  accounting:
    resources:
      limits:
        memory: 1Gi
        ephemeral-storage: "500Mi"
      requests:
        memory: 512Mi
        ephemeral-storage: "250Mi"

  product-catalog:
    envOverrides:
      - name: USE_DATABASE
        value: "true"
      - name: DB_CONNECTION_STRING
        value: "host=postgresql port=5432 user=root password=otel dbname=otel sslmode=disable"
    resources:
      limits:
        memory: 512Mi
        cpu: 512m
        ephemeral-storage: "500Mi"
      requests:
        memory: 256Mi
        cpu: 200m
        ephemeral-storage: "250Mi"

  postgresql:
    resources:
      limits:
        memory: 512Mi
        cpu: 512m
        ephemeral-storage: "1Gi"
      requests:
        memory: 256Mi
        cpu: 200m
        ephemeral-storage: "500Mi"
    # Optional: Reduce shared_buffers for IOPS pressure demo
    # Lower cache = more disk reads = visible IOPS issues
    # envOverrides:
    #   - name: POSTGRES_SHARED_BUFFERS
    #     value: "32MB"  # Default: 128MB. Use 32MB or 64MB for chaos demo

  frontend:
    envOverrides:
      - name: PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
        value: https://otel-demo.spicy.honeydemo.io/otlp-http/v1/traces

  frontend-proxy:
    service:
      type: NodePort
    ingress:
      enabled: true
      annotations:
        alb.ingress.kubernetes.io/scheme: internet-facing
        alb.ingress.kubernetes.io/healthcheck-interval-seconds: '300'
        alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTP":80}]'
        alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:465062709130:certificate/acd6cd1d-35fc-4f7b-8191-cefed94c9f69
      ingressClassName: alb
      hosts:
        - host: otel-demo.spicy.honeydemo.io
          paths:
            - path: /
              pathType: Prefix
              port: 8080


opentelemetry-collector:
  clusterRole:
    create: true
    rules:
      - apiGroups: [""]
        resources: ["nodes/stats", "nodes/proxy"]
        verbs: ["get", "list", "watch"]
      - apiGroups: [""]
        resources: ["nodes", "pods", "replicationcontrollers", "resourcequotas", "services", "events"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["events.k8s.io"]
        resources: ["events"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["batch"]
        resources: ["jobs", "cronjobs"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["autoscaling"]
        resources: ["horizontalpodautoscalers"]
        verbs: ["get", "list", "watch"]
  extraEnvs:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  config:
    receivers:
      kubeletstats:
        collection_interval: 20s
        auth_type: "serviceAccount"
        endpoint: "https://${env:K8S_NODE_NAME}:10250"
        insecure_skip_verify: true
        metric_groups:
          - pod
          - node
          - container
      k8s_cluster:
        collection_interval: 30s
        node_conditions_to_report: ["Ready", "MemoryPressure", "DiskPressure"]
        allocatable_types_to_report: ["cpu", "memory"]
        metrics:
          k8s.container.restarts:
            enabled: true
          k8s.pod.phase:
            enabled: true
          k8s.deployment.desired:
            enabled: true
          k8s.deployment.available:
            enabled: true
      prometheus:
        config:
          scrape_configs:
            - job_name: 'istio-mesh'
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                # Only scrape pods with istio-proxy container
                - source_labels: [__meta_kubernetes_pod_container_name]
                  action: keep
                  regex: istio-proxy
                # Get pod IP and set to port 15020
                - source_labels: [__meta_kubernetes_pod_ip]
                  action: replace
                  target_label: __address__
                  replacement: $1:15020
                # Add namespace label
                - source_labels: [__meta_kubernetes_namespace]
                  target_label: namespace
                # Add pod name label
                - source_labels: [__meta_kubernetes_pod_name]
                  target_label: pod
                # Add pod labels as metric labels
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
              metrics_path: /stats/prometheus
              scrape_interval: 30s
            
            - job_name: 'istiod'
              kubernetes_sd_configs:
                - role: pod
                  namespaces:
                    names:
                      - istio-system
              relabel_configs:
                # Only scrape istiod pods
                - source_labels: [__meta_kubernetes_pod_label_app]
                  action: keep
                  regex: istiod
                # Get pod IP and set to port 15014 (istiod metrics port)
                - source_labels: [__meta_kubernetes_pod_ip]
                  action: replace
                  target_label: __address__
                  replacement: $1:15014
                # Add namespace label
                - source_labels: [__meta_kubernetes_namespace]
                  target_label: namespace
                # Add pod name label
                - source_labels: [__meta_kubernetes_pod_name]
                  target_label: pod
                # Add pod labels as metric labels
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
              metrics_path: /metrics
              scrape_interval: 30s
      k8sobjects:
        auth_type: serviceAccount
        objects:
          - name: pods
            mode: watch
            group: ""
            exclude_watch_type:
              - DELETED
          - name: events
            mode: watch
            group: events.k8s.io
            namespaces: [otel-demo]
    processors:
      resource/prometheus:
        attributes:
          - key: job
            action: upsert
            from_attribute: service.name
      k8sattributes:
        passthrough: true
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.statefulset.name
            - k8s.daemonset.name
            - k8s.cronjob.name
            - k8s.job.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.pod.start_time
        pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.ip
          - sources:
            - from: resource_attribute
              name: k8s.pod.uid
          - sources:
            - from: connection
      resource/k8s_events:
        attributes:
          - key: honeycomb.dataset
            value: k8s-events
            action: upsert
      transform/k8s_events:
        log_statements:
          - context: log
            statements:
              # Extract Event object fields (for Started, BackOff, etc.)
              - set(attributes["k8s.object.kind"], body["object"]["kind"])
              - set(attributes["k8s.event.reason"], body["object"]["reason"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.event.type"], body["object"]["type"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.event.message"], body["object"]["note"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.pod.name"], body["object"]["regarding"]["name"]) where body["object"]["kind"] == "Event"
              - set(attributes["k8s.namespace.name"], body["object"]["regarding"]["namespace"]) where body["object"]["kind"] == "Event"
              
              # Extract Pod object fields (for OOMKilled, crashes, restarts)
              - set(attributes["k8s.pod.name"], body["object"]["metadata"]["name"]) where body["object"]["kind"] == "Pod"
              - set(attributes["k8s.namespace.name"], body["object"]["metadata"]["namespace"]) where body["object"]["kind"] == "Pod"
              - set(attributes["k8s.pod.phase"], body["object"]["status"]["phase"]) where body["object"]["kind"] == "Pod"
              
              # Extract container termination info (OOMKilled, Error, etc.)
              - set(attributes["k8s.container.name"], body["object"]["status"]["containerStatuses"][0]["name"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"] != nil
              - set(attributes["k8s.container.restart_count"], body["object"]["status"]["containerStatuses"][0]["restartCount"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"] != nil
              - set(attributes["k8s.container.ready"], body["object"]["status"]["containerStatuses"][0]["ready"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"] != nil
              
              # Current state
              - set(attributes["k8s.container.state"], body["object"]["status"]["containerStatuses"][0]["state"]["waiting"]["reason"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["state"]["waiting"] != nil
              - set(attributes["k8s.container.state"], body["object"]["status"]["containerStatuses"][0]["state"]["terminated"]["reason"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["state"]["terminated"] != nil
              - set(attributes["k8s.container.state"], "Running") where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["state"]["running"] != nil
              
              # Last termination (OOMKilled shows up here!)
              - set(attributes["k8s.container.last_terminated.reason"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["reason"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
              - set(attributes["k8s.container.last_terminated.exit_code"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["exitCode"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
              - set(attributes["k8s.container.last_terminated.message"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["message"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
              - set(attributes["k8s.container.last_terminated.finished_at"], body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"]["finishedAt"]) where body["object"]["kind"] == "Pod" and body["object"]["status"]["containerStatuses"][0]["lastState"]["terminated"] != nil
    exporters:
      otlp/honeycomb:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-team: f1xvILrABDNS1T6HKFvaCB
          x-honeycomb-dataset: opentelemetry-demo
      otlp/honeycomb_events:
        endpoint: api.honeycomb.io:443
        headers:
          x-honeycomb-team: f1xvILrABDNS1T6HKFvaCB
          x-honeycomb-dataset: k8s-events
    service:
      pipelines:
        logs:
          exporters:
          - otlp/honeycomb
          - debug
        logs/k8sevents:
          receivers:
          - k8sobjects
          processors:
          - resource/k8s_events
          - transform/k8s_events
          exporters:
          - otlp/honeycomb_events
          - debug
        metrics:
          receivers:
          - otlp
          - spanmetrics
          - kubeletstats
          - k8s_cluster
          - postgresql
          - prometheus
          processors:
          - k8sattributes
          - resource/prometheus
          exporters:
          - otlp/honeycomb
          - debug
        traces:
          exporters:
          - otlp/honeycomb
          - spanmetrics
          - debug

prometheus:
  server:
    resources:
      limits:
        memory: "2Gi"
        cpu: "1000m"
        ephemeral-storage: "2Gi"
      requests:
        memory: "1Gi"
        cpu: "300m"
        ephemeral-storage: "1Gi"
