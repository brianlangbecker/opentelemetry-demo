# PostgreSQL Connection Exhaustion Test Results

**Test Date:** November 7, 2025  
**Environment:** OpenTelemetry Demo on Kubernetes  
**Test Type:** Connection Pool Exhaustion Chaos Engineering

---

## Test Configuration

### PostgreSQL Settings

- **max_connections:** 100
- **superuser_reserved_connections:** 3
- **Database:** otel (persistent storage via PVC)
- **Memory limit:** 512Mi
- **Persistent storage:** 2Gi PVC (configured via `infra/install-with-persistence.sh`)

### Test Parameters

- **Test scenario:** `connection-exhaust` via `infra/postgres-chaos/postgres-chaos-scenarios.sh`
- **Connections held:** 92 (92% of max_connections)
- **Duration:** 5 minutes (300 seconds)
- **Connection closure:** 7-9 existing connections closed at test start
- **Tests executed:** 2 successful runs

---

## System State During Tests

### Database Content

- **Products:** 1,510 (persistent across tests)
- **Orders processed:** 168,050+ (Test #1) ‚Üí 168,495 (Test #2)
- **Table sizes:**
  - products: 8MB
  - order, orderitem, shipping: Active
  - pgbench tables: Present from prior testing

### Services Configuration

- **Kafka:** Working (upgraded from 600Mi ‚Üí 1536Mi memory, heap: 1024M)
- **Load Generator:** Active with browser traffic enabled
- **Accounting:** 256Mi memory (upgraded from 120Mi to prevent OOM)
- **Product-Catalog:**
  - MaxOpenConns: 25
  - MaxIdleConns: 5
  - ConnMaxLifetime: 5 minutes
  - USE_DATABASE: true

---

## Test #1 Results - Initial Run

### Execution

```bash
cd /Users/brianlangbecker/Documents/GitHub/opentelemetry-demo
echo "y" | ./infra/postgres-chaos/postgres-chaos-scenarios.sh connection-exhaust
```

### Connection Statistics

- **Closed at start:** 9 connections
- **Peak connections:** 102/100 (over limit)
- **Test duration:** Full 5 minutes
- **Orders during test:** 12,735 created (despite connection pressure)

### Blast Radius

#### üî¥ **Accounting Service - CRITICAL FAILURE**

- **Error:** `FATAL: sorry, too many clients already` (PostgreSQL error 53300)
- **Occurrences:** Multiple instances
- **Impact:** Npgsql connection pool exhaustion, Entity Framework failures
- **Effect:** Unable to persist incoming Kafka messages to database

#### üî¥ **Product-Catalog - IMPACTED**

- **Errors:** 3 instances of `failed to query product: pq: sorry, too many clients already`
- **Impact:** Database queries failed when pool exhausted
- **Propagation:** Errors bubbled to frontend as `2 UNKNOWN` gRPC errors
- **Note:** Go's database/sql package logs errors at non-ERROR level, making them less visible

#### üü° **Load Generator / Frontend - DEGRADED**

- **Errors:** Multiple `500 Internal Server Error` responses
- **Additional:** `ERR_CONNECTION_REFUSED` errors
- **Impact:** User-facing transaction failures

#### ‚úÖ **System Recovery**

- Immediate recovery after test completion
- All services reconnected successfully
- No manual intervention required

---

## Test #2 Results - Verification Run

### Execution

Same command, executed ~15 minutes after Test #1 completed

### Connection Statistics

- **Closed at start:** 7 connections
- **Peak connections:** Held at limit for full duration
- **Test duration:** Full 5 minutes
- **Orders during test:** 445 created (96% reduction vs Test #1)

### Blast Radius

#### üî¥ **Accounting Service**

- **Errors:** 11 instances of "too many clients already"
- **Impact:** Severely reduced order processing capacity
- **Comparison:** 445 orders vs 12,735 in Test #1 (96% reduction)

#### üî¥ **Product-Catalog**

- **Errors:** Database connection failures
- **Evidence:** Frontend showing product query failures
- **Impact:** Product listings and checkout product validation affected

#### üî¥ **Load Generator / Frontend**

- **Errors:** `503 Service Unavailable` responses
- **Additional:** `ERR_CONNECTION_REFUSED` errors
- **Pattern:** Similar to Test #1, consistent blast radius

#### ‚úÖ **Data Persistence**

- **Products:** All 1,510 products intact after both tests
- **Validation:** Persistent storage working correctly

---

## Key Findings

### 1. Connection Pool Behavior

- **Go services (product-catalog):** Silent retry in connection pool, errors propagate as application errors
- **C# services (accounting):** Explicit Npgsql exceptions, clear error messages
- Services with **pre-established connections** survived longer
- Services needing **new connections** failed immediately

### 2. Blast Radius Pattern

```
PostgreSQL (92/100 connections held)
    ‚Üì
Accounting: Cannot write orders (FATAL: too many clients)
    ‚Üì
Product-Catalog: Cannot read products (connection refused)
    ‚Üì
Frontend: 500/503 errors ‚Üí User-facing failures
```

### 3. Differences Between Test Runs

| Metric            | Test #1 | Test #2        | Analysis          |
| ----------------- | ------- | -------------- | ----------------- |
| Accounting Errors | Many    | 11             | Lower frequency   |
| Orders Created    | 12,735  | 445            | 96% reduction     |
| Product Errors    | 3       | Multiple       | Consistent impact |
| Load Gen Errors   | 500s    | 503s + refused | Similar patterns  |

**Hypothesis for differences:** Test #2 may have had lower overall system load or services more quickly exhausted retry attempts.

### 4. Service-Specific Observations

**Accounting (C#/.NET):**

- Uses Npgsql with Entity Framework
- Explicit error messages in logs
- Connection pool visible in exception traces
- Failed fast when pool exhausted

**Product-Catalog (Go):**

- Uses database/sql with lib/pq driver
- Errors logged at INFO level, not ERROR
- Silent retries in connection pool
- Errors only visible in calling services (frontend)

**Checkout (Go):**

- No direct database connection
- Impact via dependencies (product-catalog for product validation)
- Indirect failures manifested as timeouts/503s

---

## Test Validation

### ‚úÖ Confirmed Behaviors

1. **Connection exhaustion achieved** - Successfully reached max_connections limit
2. **Multi-service impact** - Both direct database consumers affected
3. **Error propagation** - Failures cascaded through service dependencies
4. **Immediate recovery** - System self-healed after connections released
5. **Data persistence** - Products and database state survived tests
6. **No false positives** - Errors correlated directly with test execution

### ‚úÖ Monitoring Effectiveness

- Connection count tracking visible (104/100 at peak)
- Service-specific error detection working
- Blast radius observable across service boundaries
- Recovery time measurable

---

## Comparison with Other Scenarios

| Scenario            | Connection Exhaustion     | Table Lock                  | Slow Query              |
| ------------------- | ------------------------- | --------------------------- | ----------------------- |
| **Symptom**         | Connection errors         | Query timeouts              | Slow queries (all)      |
| **CPU**             | Low                       | Low                         | High (80-100%)          |
| **Error Type**      | "too many clients"        | Timeout/blocked             | Latency, no errors      |
| **Accounting**      | üî• Severe                 | üî• Severe                   | ‚ö†Ô∏è Moderate             |
| **Product-Catalog** | üî• Severe                 | üî• Severe                   | ‚ö†Ô∏è Moderate             |
| **Impact Pattern**  | Symmetric (both services) | Asymmetric (table-specific) | Symmetric (all queries) |

---

## Recommendations

### For Observability

1. ‚úÖ Monitor `db.connection.count` / `db.connection.max` ratio
2. ‚úÖ Alert when >90% connection utilization
3. ‚úÖ Track connection errors by service
4. ‚úÖ Correlate with application error rates
5. ‚úÖ Monitor recovery time after incidents

### For System Architecture

1. **Connection Pooling:**

   - Product-catalog: MaxOpenConns: 25 is appropriate for this workload
   - Accounting: Entity Framework pool sizing adequate
   - Consider circuit breakers for connection failures

2. **Capacity Planning:**

   - Current: max_connections=100 adequate for normal load
   - Under test: Services handle pool exhaustion gracefully
   - Recommendation: Monitor connection usage trends

3. **Error Handling:**
   - Product-catalog: Consider more visible error logging for database issues
   - Accounting: Current error visibility is good
   - Frontend: Proper error propagation working

### For Testing

1. ‚úÖ Test documented and reproducible
2. ‚úÖ Safe to run in staging environments
3. ‚úÖ Blast radius well-understood
4. ‚ö†Ô∏è Consider scheduling (avoid during heavy load)

---

## Related Tests Available

From `infra/postgres-chaos/postgres-chaos-scenarios.sh`:

1. **table-lock** - Locks order/products tables for 10 minutes
2. **slow-query** - CPU saturation with expensive queries
3. **accounting** - Targets real application tables with pgbench
4. **light/normal/beast** - Various pgbench load levels

---

## Files Modified/Created During Testing

### Configuration Changes

- `infra/postgres-chaos/postgres-chaos-scenarios.sh` - Updated to 92 connections
- `infra/postgres-chaos/generate-products.sql` - Configured for 1500 products
- `docker-compose.yml` - Kafka memory: 620M ‚Üí 1536M, heap: 400m ‚Üí 1024m
- `kubernetes/opentelemetry-demo.yaml` - Kafka memory: 600Mi ‚Üí 1536Mi

### Infrastructure Setup

- Applied persistent storage patch: `infra/postgres-patch.yaml`
- Created 1,510 products using `infra/postgres-chaos/generate-products.sh`

### System Fixes During Testing

1. **Kafka memory increase** - Resolved broker restart issues
2. **PostgreSQL persistent storage** - Enabled data persistence across restarts
3. **Products table** - Created and seeded (was missing initially)
4. **Product count adjustment** - Reduced from 11,010 to 1,510 to avoid gRPC message size limits

---

## Reproduction Steps

### Prerequisites

```bash
# 1. Ensure PostgreSQL has persistent storage
cd /Users/brianlangbecker/Documents/GitHub/opentelemetry-demo/infra
kubectl patch deployment postgresql -n otel-demo --patch-file postgres-patch.yaml

# 2. Verify products table exists with reasonable count (1,000-2,000)
kubectl exec -n otel-demo <postgres-pod> -- psql -U root otel -c "SELECT COUNT(*) FROM products;"

# 3. Ensure Kafka is healthy with adequate resources
kubectl get pods -n otel-demo | grep kafka  # Should show 2/2 Running

# 4. Verify load generator is active
kubectl get pods -n otel-demo | grep load-generator  # Should show 2/2 Running
```

### Execute Test

```bash
cd /Users/brianlangbecker/Documents/GitHub/opentelemetry-demo
echo "y" | ./infra/postgres-chaos/postgres-chaos-scenarios.sh connection-exhaust
```

### Monitor During Test

```bash
# Watch connection count
watch -n 10 'kubectl exec -n otel-demo <postgres-pod> -- psql -U root otel -c "SELECT COUNT(*) FROM pg_stat_activity WHERE datname='"'"'otel'"'"';"'

# Check service errors
kubectl logs -n otel-demo -l app.kubernetes.io/component=accounting --tail=20 | grep "too many clients"
kubectl logs -n otel-demo -l app.kubernetes.io/component=frontend --tail=20 | grep "failed to query product"
```

### Verify Recovery

```bash
# Check connection count returns to normal (<20)
kubectl exec -n otel-demo <postgres-pod> -- psql -U root otel -c "SELECT COUNT(*) FROM pg_stat_activity WHERE datname='otel';"

# Verify services recovered
kubectl get pods -n otel-demo  # All should be Running
```

---

## Honeycomb Queries

### Connection Count

```
Dataset: Metrics
WHERE service.name = "postgresql"
Calculate: MAX(db.connection.count)
Time: Last 10 minutes
```

### Accounting Errors

```
Dataset: accounting
WHERE exception.message CONTAINS "too many clients"
Calculate: COUNT
Time: Last 10 minutes
```

### Product-Catalog Impact

```
Dataset: frontend
WHERE error CONTAINS "failed to query product"
Calculate: COUNT
Breakdown: error
Time: Last 10 minutes
```

### Checkout Service Health

```
Dataset: checkout
WHERE rpc.service = "oteldemo.ProductCatalogService"
Calculate: COUNT, P95(duration_ms)
Breakdown: http.status_code
Time: Last 10 minutes
```

---

## Test Artifacts

### Logs Captured

- Accounting: Multiple "too many clients" errors with full stack traces
- Product-catalog: Database connection failures (visible in frontend logs)
- Frontend: 500/503 errors with gRPC status codes
- PostgreSQL: Connection count at 102-104 during test peak

### Metrics Observed

- Connection utilization: 102-104/100 (sustained)
- Order processing: 445-12,735 orders during 5-minute window
- Error rates: High in accounting, product-catalog during test
- Recovery time: <30 seconds after test completion

---

## Conclusion

The connection exhaustion tests successfully demonstrated:

1. ‚úÖ **Reproducible blast radius** across two test runs
2. ‚úÖ **Multi-service impact** affecting both database consumers
3. ‚úÖ **Clear error signatures** distinguishing from other failure modes
4. ‚úÖ **Effective monitoring** capability via connection counts and error logs
5. ‚úÖ **Graceful degradation** with no service crashes
6. ‚úÖ **Fast recovery** without manual intervention
7. ‚úÖ **Data persistence** maintaining system state

**Test Status:** ‚úÖ **VALIDATED** - Ready for chaos engineering demonstrations

---

**Documentation Author:** Test Results from November 7, 2025 testing session  
**Script Location:** `infra/postgres-chaos/postgres-chaos-scenarios.sh`  
**Related Docs:** `infra/postgres-chaos/docs/POSTGRES-CHAOS-SCENARIOS.md`
